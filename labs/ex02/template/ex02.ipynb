{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.630035440188782"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    M = y - tx @ w\n",
    "    M = abs(M)\n",
    "\n",
    "    \n",
    "    return 1/N*np.sum(M)\n",
    "\n",
    "compute_loss(y, tx, np.array([73,12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([grid_w0[i], grid_w1[j]]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=4.8736571178918675, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.272 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    return -1/N*(tx.T @ (y - tx @ w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=73.29392200210518, w0=7.329392200210517, w1=1.3479712434988966\n",
      "GD iter. 1/49: loss=65.96452980189467, w0=13.925845180399982, w1=2.5611453626479044\n",
      "GD iter. 2/49: loss=59.3680768217052, w0=19.862652862570503, w1=3.6530020698820107\n",
      "GD iter. 3/49: loss=53.43126913953468, w0=25.205779776523972, w1=4.63567310639271\n",
      "GD iter. 4/49: loss=48.088142225581215, w0=30.014593999082095, w1=5.520077039252339\n",
      "GD iter. 5/49: loss=43.27932800302308, w0=34.3425267993844, w1=6.316040578826005\n",
      "GD iter. 6/49: loss=38.95139520272078, w0=38.23766631965648, w1=7.032407764442305\n",
      "GD iter. 7/49: loss=35.05625568244869, w0=41.74329188790135, w1=7.677138231496976\n",
      "GD iter. 8/49: loss=31.550630114203834, w0=44.89835489932173, w1=8.25739565184618\n",
      "GD iter. 9/49: loss=28.395567102783453, w0=47.737911609600076, w1=8.779627330160464\n",
      "GD iter. 10/49: loss=25.55601039250511, w0=50.29351264885059, w1=9.24963584064332\n",
      "GD iter. 11/49: loss=23.00076772921491, w0=52.59355358417605, w1=9.672643500077891\n",
      "GD iter. 12/49: loss=20.70177123207136, w0=54.66359042596896, w1=10.053350393569005\n",
      "GD iter. 13/49: loss=18.63324309146752, w0=56.526623583582584, w1=10.395986597711007\n",
      "GD iter. 14/49: loss=16.773413045768155, w0=58.20335342543484, w1=10.704359181438809\n",
      "GD iter. 15/49: loss=15.105079537477698, w0=59.712410283101875, w1=10.981894506793832\n",
      "GD iter. 16/49: loss=13.614072467918849, w0=61.070561455002206, w1=11.231676299613353\n",
      "GD iter. 17/49: loss=12.287214020480024, w0=62.2928975097125, w1=11.456479913150922\n",
      "GD iter. 18/49: loss=11.113409318159817, w0=63.39299995895177, w1=11.658803165334733\n",
      "GD iter. 19/49: loss=10.08353045920831, w0=64.38309216326711, w1=11.840894092300163\n",
      "GD iter. 20/49: loss=9.188125911100752, w0=65.27417514715091, w1=12.004775926569051\n",
      "GD iter. 21/49: loss=8.416279392143387, w0=66.07614983264634, w1=12.152269577411051\n",
      "GD iter. 22/49: loss=7.754809954187993, w0=66.79792704959222, w1=12.28501386316885\n",
      "GD iter. 23/49: loss=7.188299871521944, w0=67.44752654484351, w1=12.40448372035087\n",
      "GD iter. 24/49: loss=6.70912616952619, w0=68.03216609056967, w1=12.512006591814687\n",
      "GD iter. 25/49: loss=6.305154796854909, w0=68.55834168172322, w1=12.608777176132122\n",
      "GD iter. 26/49: loss=5.9671566321191305, w0=69.03189971376142, w1=12.695870702017814\n",
      "GD iter. 27/49: loss=5.68545374921709, w0=69.45810194259579, w1=12.774254875314936\n",
      "GD iter. 28/49: loss=5.452412516712416, w0=69.84168394854673, w1=12.844800631282347\n",
      "GD iter. 29/49: loss=5.260854661605658, w0=70.18690775390257, w1=12.908291811653017\n",
      "GD iter. 30/49: loss=5.10522641949504, w0=70.49760917872284, w1=12.965433873986619\n",
      "GD iter. 31/49: loss=4.977419147187544, w0=70.77724046106107, w1=13.016861730086863\n",
      "GD iter. 32/49: loss=4.872507667329794, w0=71.02890861516548, w1=13.063146800577082\n",
      "GD iter. 33/49: loss=4.7867117697079635, w0=71.25540995385944, w1=13.104803364018277\n",
      "GD iter. 34/49: loss=4.716903792721049, w0=71.45926115868401, w1=13.142294271115354\n",
      "GD iter. 35/49: loss=4.660996544536756, w0=71.64272724302613, w1=13.176036087502723\n",
      "GD iter. 36/49: loss=4.615683065166653, w0=71.80784671893403, w1=13.206403722251356\n",
      "GD iter. 37/49: loss=4.579124620087087, w0=71.95645424725114, w1=13.233734593525124\n",
      "GD iter. 38/49: loss=4.54919099854465, w0=72.09020102273655, w1=13.258332377671517\n",
      "GD iter. 39/49: loss=4.524879944009209, w0=72.21057312067342, w1=13.28047038340327\n",
      "GD iter. 40/49: loss=4.505146131687975, w0=72.31890800881659, w1=13.300394588561847\n",
      "GD iter. 41/49: loss=4.489335490515159, w0=72.41640940814546, w1=13.318326373204567\n",
      "GD iter. 42/49: loss=4.476572343915047, w0=72.50416066754143, w1=13.334464979383014\n",
      "GD iter. 43/49: loss=4.466283086575103, w0=72.5831368009978, w1=13.348989724943618\n",
      "GD iter. 44/49: loss=4.458044092535258, w0=72.65421532110854, w1=13.36206199594816\n",
      "GD iter. 45/49: loss=4.451377893334732, w0=72.71818598920821, w1=13.373827039852248\n",
      "GD iter. 46/49: loss=4.445950686305893, w0=72.77575959049791, w1=13.384415579365928\n",
      "GD iter. 47/49: loss=4.441709425498295, w0=72.82757583165863, w1=13.39394526492824\n",
      "GD iter. 48/49: loss=4.4383201883459815, w0=72.8742104487033, w1=13.40252198193432\n",
      "GD iter. 49/49: loss=4.435552205084079, w0=72.91618160404349, w1=13.410241027239794\n",
      "GD: execution time=0.020 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe2c49d79164b00bb247d677a8333e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 50):\n",
    "        batch_tx = minibatch_tx\n",
    "        batch_y = minibatch_y\n",
    "\n",
    "    return compute_gradient(batch_y, batch_tx, w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_stoch_gradient(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=73.29392200210518, w0=7.062461304141848, w1=0.19008059156954443\n",
      "SGD iter. 1/49: loss=66.23146069796334, w0=13.647373116693387, w1=1.096688016589131\n",
      "SGD iter. 2/49: loss=59.6465488854118, w0=19.452913339247207, w1=1.5046751256554727\n",
      "SGD iter. 3/49: loss=53.84100866285798, w0=25.041506018143675, w1=2.9518784146531942\n",
      "SGD iter. 4/49: loss=48.2524159839615, w0=30.074282460575642, w1=4.911500906821737\n",
      "SGD iter. 5/49: loss=43.21963954152954, w0=34.47034356252487, w1=6.483717399137265\n",
      "SGD iter. 6/49: loss=38.82357843958032, w0=38.465145584998744, w1=7.6107193507049775\n",
      "SGD iter. 7/49: loss=34.82877641710644, w0=41.96269275228088, w1=8.288358594671521\n",
      "SGD iter. 8/49: loss=31.331229249824304, w0=45.160462178817674, w1=8.64737022029695\n",
      "SGD iter. 9/49: loss=28.133459823287513, w0=48.05072778847739, w1=8.97817242332928\n",
      "SGD iter. 10/49: loss=25.243194213627795, w0=50.6135076622766, w1=9.729087434730243\n",
      "SGD iter. 11/49: loss=22.68080918648248, w0=52.926868747381775, w1=10.164672161130039\n",
      "SGD iter. 12/49: loss=20.368506754215208, w0=54.9892770656848, w1=10.114256222810676\n",
      "SGD iter. 13/49: loss=18.307883917140526, w0=56.74149029138655, w1=10.62230371870248\n",
      "SGD iter. 14/49: loss=16.55885615414614, w0=58.44751974771798, w1=10.71467583649547\n",
      "SGD iter. 15/49: loss=14.863048738585606, w0=60.05012818973229, w1=11.081623052849682\n",
      "SGD iter. 16/49: loss=13.282233679829064, w0=61.34807455687002, w1=11.35341675392895\n",
      "SGD iter. 17/49: loss=12.017494181945409, w0=62.61142595404778, w1=11.300138687329987\n",
      "SGD iter. 18/49: loss=10.818291469131932, w0=63.58572119172944, w1=11.801273292426712\n",
      "SGD iter. 19/49: loss=9.902426147225622, w0=64.62008086085727, w1=12.117361151791863\n",
      "SGD iter. 20/49: loss=8.96768904345027, w0=65.36179512219745, w1=12.308119749076306\n",
      "SGD iter. 21/49: loss=8.325874162389384, w0=66.1220643811652, w1=12.309920702012995\n",
      "SGD iter. 22/49: loss=7.707273866622523, w0=66.94951212948851, w1=12.43278110892299\n",
      "SGD iter. 23/49: loss=7.064325747766147, w0=67.71116960370173, w1=12.32940312155215\n",
      "SGD iter. 24/49: loss=6.535140308120743, w0=68.33266314218646, w1=12.283844669067555\n",
      "SGD iter. 25/49: loss=6.139783828060206, w0=68.79944439503883, w1=12.33583268001716\n",
      "SGD iter. 26/49: loss=5.858966285760203, w0=69.33593507665114, w1=12.450591051987193\n",
      "SGD iter. 27/49: loss=5.55323031071683, w0=69.72674427802443, w1=12.40844891019196\n",
      "SGD iter. 28/49: loss=5.368409935944556, w0=69.93647524653824, w1=12.510322386149113\n",
      "SGD iter. 29/49: loss=5.258641808842702, w0=70.07573448147286, w1=12.599965856352073\n",
      "SGD iter. 30/49: loss=5.1860989580962835, w0=70.41565605792118, w1=12.751968318713441\n",
      "SGD iter. 31/49: loss=5.03029769453119, w0=70.71474030302151, w1=13.006448186998435\n",
      "SGD iter. 32/49: loss=4.895045365108272, w0=70.93445103651494, w1=13.125670588946425\n",
      "SGD iter. 33/49: loss=4.811716186419691, w0=71.23778686005829, w1=13.237556964357017\n",
      "SGD iter. 34/49: loss=4.713548335545009, w0=71.29914504531484, w1=13.30360553275904\n",
      "SGD iter. 35/49: loss=4.693523245916141, w0=71.5252809339766, w1=13.392462713319622\n",
      "SGD iter. 36/49: loss=4.633250101760278, w0=71.76322140642658, w1=13.412995000442184\n",
      "SGD iter. 37/49: loss=4.579499847129206, w0=71.94784425233868, w1=13.408266972440753\n",
      "SGD iter. 38/49: loss=4.543628629272113, w0=72.02026820920862, w1=13.41826633471975\n",
      "SGD iter. 39/49: loss=4.530619235406799, w0=72.22617508100292, w1=13.376805260508942\n",
      "SGD iter. 40/49: loss=4.499117623380642, w0=72.34730307440991, w1=13.413429046664787\n",
      "SGD iter. 41/49: loss=4.481702416394165, w0=72.66347243679324, w1=13.346979787176375\n",
      "SGD iter. 42/49: loss=4.45111143751703, w0=72.70065718127903, w1=13.303430453585324\n",
      "SGD iter. 43/49: loss=4.449817709968658, w0=72.76187219512502, w1=13.31250169459848\n",
      "SGD iter. 44/49: loss=4.44499608393418, w0=72.88517888263824, w1=13.425723100705806\n",
      "SGD iter. 45/49: loss=4.434569688668391, w0=72.83989304382656, w1=13.40687616513963\n",
      "SGD iter. 46/49: loss=4.4373088337388955, w0=72.82282310249921, w1=13.451168654701382\n",
      "SGD iter. 47/49: loss=4.4374762024153185, w0=72.92453661701624, w1=13.453869533508877\n",
      "SGD iter. 48/49: loss=4.43226808005376, w0=72.85428858822206, w1=13.505352569964675\n",
      "SGD iter. 49/49: loss=4.435206389377259, w0=72.868259287562, w1=13.388499020115548\n",
      "SGD: execution time=0.027 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d21721ef4b4ee585edba53e10520e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=74.06780585492638, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=22.220341756477886, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=8.540676439415343, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=6.904828366810239, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=6.674709716875216, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=6.644814813344382, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=6.639117501635084, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=6.637802926368862, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=6.637503390841276, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=6.637417848906234, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=6.637392186325722, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=6.637384487551568, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=6.637382177919323, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=6.637381485029651, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=6.6373812771627465, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=6.6373812148026765, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=6.637381196094654, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=6.637381190482249, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=6.637381188798527, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=6.637381188293409, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=6.637381188141875, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=6.637381188096415, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=6.637381188082776, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=6.637381188078685, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=6.637381188077455, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=6.63738118807709, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=6.637381188076979, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=6.6373811880769455, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=6.637381188076935, w0=74.06780585492632, w1=11.034894865989095\n",
      "GD iter. 29/49: loss=6.637381188076931, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=6.6373811880769304, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=6.637381188076931, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c01f54f963040028f5ce4cea20b8c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    loss = y - tx @ w\n",
    "    N = len(y)\n",
    "    subgradient = -1/N*np.array([np.sum(np.sign(loss)),-np.sum(x[1]*np.sign(loss))])\n",
    "    return subgradient\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_subgradient_mae(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=0.24097991466664806\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=0.4819598293332961\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=0.7229397439999442\n",
      "SubGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=0.9639196586665922\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=1.2048995733332404\n",
      "SubGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=1.4458794879998884\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=1.6868594026665364\n",
      "SubGD iter. 7/499: loss=69.16780585492637, w0=5.6000000000000005, w1=1.9278393173331845\n",
      "SubGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=2.1688192319998327\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=2.4097991466664808\n",
      "SubGD iter. 10/499: loss=67.06780585492636, w0=7.700000000000001, w1=2.650779061333129\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=2.891758975999777\n",
      "SubGD iter. 12/499: loss=65.66780585492637, w0=9.1, w1=3.132738890666425\n",
      "SubGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=3.373718805333073\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=3.614698719999721\n",
      "SubGD iter. 15/499: loss=63.56780585492638, w0=11.199999999999998, w1=3.855678634666369\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=4.096658549333017\n",
      "SubGD iter. 17/499: loss=62.16780585492638, w0=12.599999999999996, w1=4.3376384639996655\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=4.578618378666314\n",
      "SubGD iter. 19/499: loss=60.76780585492637, w0=13.999999999999995, w1=4.819598293332962\n",
      "SubGD iter. 20/499: loss=60.06780585492637, w0=14.699999999999994, w1=5.060578207999611\n",
      "SubGD iter. 21/499: loss=59.367805854926374, w0=15.399999999999993, w1=5.301558122666259\n",
      "SubGD iter. 22/499: loss=58.66780585492637, w0=16.099999999999994, w1=5.542538037332908\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=5.783517951999556\n",
      "SubGD iter. 24/499: loss=57.26780585492637, w0=17.499999999999993, w1=6.024497866666205\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=6.265477781332853\n",
      "SubGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=6.506457695999502\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=6.74743761066615\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=6.988417525332799\n",
      "SubGD iter. 29/499: loss=53.76780585492637, w0=20.99999999999999, w1=7.229397439999447\n",
      "SubGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=7.470377354666096\n",
      "SubGD iter. 31/499: loss=52.367805854926374, w0=22.399999999999988, w1=7.711357269332744\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=7.952337183999393\n",
      "SubGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=8.193317098666041\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=8.434297013332689\n",
      "SubGD iter. 35/499: loss=49.567805854926384, w0=25.199999999999985, w1=8.675276927999336\n",
      "SubGD iter. 36/499: loss=48.867805854926374, w0=25.899999999999984, w1=8.916256842665984\n",
      "SubGD iter. 37/499: loss=48.167805854926385, w0=26.599999999999984, w1=9.157236757332631\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=9.398216671999279\n",
      "SubGD iter. 39/499: loss=46.76780585492638, w0=27.999999999999982, w1=9.639196586665927\n",
      "SubGD iter. 40/499: loss=46.067805854926384, w0=28.69999999999998, w1=9.880176501332574\n",
      "SubGD iter. 41/499: loss=45.36780585492639, w0=29.39999999999998, w1=10.121156415999222\n",
      "SubGD iter. 42/499: loss=44.667805854926385, w0=30.09999999999998, w1=10.36213633066587\n",
      "SubGD iter. 43/499: loss=43.96780585492639, w0=30.79999999999998, w1=10.603116245332517\n",
      "SubGD iter. 44/499: loss=43.267805854926394, w0=31.49999999999998, w1=10.844096159999165\n",
      "SubGD iter. 45/499: loss=42.567805854926384, w0=32.19999999999998, w1=11.085076074665812\n",
      "SubGD iter. 46/499: loss=41.867805854926374, w0=32.899999999999984, w1=11.32605598933246\n",
      "SubGD iter. 47/499: loss=41.16780585492638, w0=33.59999999999999, w1=11.567035903999107\n",
      "SubGD iter. 48/499: loss=40.467805854926375, w0=34.29999999999999, w1=11.808015818665755\n",
      "SubGD iter. 49/499: loss=39.76780585492637, w0=34.99999999999999, w1=12.048995733332402\n",
      "SubGD iter. 50/499: loss=39.06780585492637, w0=35.699999999999996, w1=12.28997564799905\n",
      "SubGD iter. 51/499: loss=38.36780585492637, w0=36.4, w1=12.530955562665698\n",
      "SubGD iter. 52/499: loss=37.667805854926364, w0=37.1, w1=12.771935477332345\n",
      "SubGD iter. 53/499: loss=36.96780585492636, w0=37.800000000000004, w1=13.012915391998993\n",
      "SubGD iter. 54/499: loss=36.26780585492636, w0=38.50000000000001, w1=13.25389530666564\n",
      "SubGD iter. 55/499: loss=35.567805854926355, w0=39.20000000000001, w1=13.494875221332288\n",
      "SubGD iter. 56/499: loss=34.86780585492635, w0=39.90000000000001, w1=13.735855135998936\n",
      "SubGD iter. 57/499: loss=34.16780585492635, w0=40.600000000000016, w1=13.976835050665583\n",
      "SubGD iter. 58/499: loss=33.46780585492635, w0=41.30000000000002, w1=14.21781496533223\n",
      "SubGD iter. 59/499: loss=32.767805854926344, w0=42.00000000000002, w1=14.458794879998878\n",
      "SubGD iter. 60/499: loss=32.06780585492634, w0=42.700000000000024, w1=14.699774794665526\n",
      "SubGD iter. 61/499: loss=31.367805854926335, w0=43.40000000000003, w1=14.940754709332174\n",
      "SubGD iter. 62/499: loss=30.667805854926332, w0=44.10000000000003, w1=15.181734623998821\n",
      "SubGD iter. 63/499: loss=29.967805854926333, w0=44.80000000000003, w1=15.422714538665469\n",
      "SubGD iter. 64/499: loss=29.26780585492633, w0=45.500000000000036, w1=15.663694453332116\n",
      "SubGD iter. 65/499: loss=28.567805854926327, w0=46.20000000000004, w1=15.904674367998764\n",
      "SubGD iter. 66/499: loss=27.867805854926324, w0=46.90000000000004, w1=16.14565428266541\n",
      "SubGD iter. 67/499: loss=27.16780585492632, w0=47.600000000000044, w1=16.38663419733206\n",
      "SubGD iter. 68/499: loss=26.46780585492632, w0=48.30000000000005, w1=16.62761411199871\n",
      "SubGD iter. 69/499: loss=25.767805854926316, w0=49.00000000000005, w1=16.86859402666536\n",
      "SubGD iter. 70/499: loss=25.067805854926313, w0=49.70000000000005, w1=17.10957394133201\n",
      "SubGD iter. 71/499: loss=24.36780585492631, w0=50.400000000000055, w1=17.35055385599866\n",
      "SubGD iter. 72/499: loss=23.667805854926307, w0=51.10000000000006, w1=17.591533770665308\n",
      "SubGD iter. 73/499: loss=22.9678058549263, w0=51.80000000000006, w1=17.832513685331957\n",
      "SubGD iter. 74/499: loss=22.267805854926298, w0=52.500000000000064, w1=18.073493599998606\n",
      "SubGD iter. 75/499: loss=21.567805854926295, w0=53.20000000000007, w1=18.314473514665256\n",
      "SubGD iter. 76/499: loss=20.867805854926285, w0=53.90000000000007, w1=18.555453429331905\n",
      "SubGD iter. 77/499: loss=20.16780585492629, w0=54.60000000000007, w1=18.796433343998554\n",
      "SubGD iter. 78/499: loss=19.467805854926286, w0=55.300000000000075, w1=19.037413258665204\n",
      "SubGD iter. 79/499: loss=18.767805854926284, w0=56.00000000000008, w1=19.278393173331853\n",
      "SubGD iter. 80/499: loss=18.067805854926277, w0=56.70000000000008, w1=19.519373087998503\n",
      "SubGD iter. 81/499: loss=17.367805854926274, w0=57.400000000000084, w1=19.760353002665152\n",
      "SubGD iter. 82/499: loss=16.66780585492627, w0=58.10000000000009, w1=20.0013329173318\n",
      "SubGD iter. 83/499: loss=15.972537657946242, w0=58.79306930693078, w1=20.239926892249276\n",
      "SubGD iter. 84/499: loss=15.298221307736041, w0=59.47227722772286, w1=20.4737489876684\n",
      "SubGD iter. 85/499: loss=14.656017268703186, w0=60.144554455445636, w1=20.70518514333835\n",
      "SubGD iter. 86/499: loss=14.032114147953697, w0=60.78910891089118, w1=20.927077540011602\n",
      "SubGD iter. 87/499: loss=13.459949333552297, w0=61.426732673267416, w1=21.14658399693568\n",
      "SubGD iter. 88/499: loss=12.924954544019766, w0=62.03663366336642, w1=21.356546694863056\n",
      "SubGD iter. 89/499: loss=12.42878689268243, w0=62.63267326732682, w1=21.561737513292083\n",
      "SubGD iter. 90/499: loss=11.969466977147071, w0=63.2079207920793, w1=21.759770512473587\n",
      "SubGD iter. 91/499: loss=11.565407703615701, w0=63.74851485148524, w1=21.945873812909216\n",
      "SubGD iter. 92/499: loss=11.197766658353185, w0=64.28910891089119, w1=22.131977113344846\n",
      "SubGD iter. 93/499: loss=10.83461120786403, w0=64.80891089108921, w1=22.31092259453295\n",
      "SubGD iter. 94/499: loss=10.51702359858166, w0=65.2871287128714, w1=22.475552437226007\n",
      "SubGD iter. 95/499: loss=10.259754851570396, w0=65.74455445544565, w1=22.63302446067154\n",
      "SubGD iter. 96/499: loss=10.031025046544672, w0=66.16039603960407, w1=22.776180845622022\n",
      "SubGD iter. 97/499: loss=9.846137507018051, w0=66.56930693069317, w1=22.91695129082333\n",
      "SubGD iter. 98/499: loss=9.671255209149841, w0=66.96435643564367, w1=23.05294985652629\n",
      "SubGD iter. 99/499: loss=9.516013282659202, w0=67.33168316831693, w1=23.17940466323255\n",
      "SubGD iter. 100/499: loss=9.383368351036815, w0=67.69207920792088, w1=23.303473530189635\n",
      "SubGD iter. 101/499: loss=9.263311813997527, w0=68.03168316831692, w1=23.420384577899195\n",
      "SubGD iter. 102/499: loss=9.160774361248095, w0=68.35049504950504, w1=23.530137806361232\n",
      "SubGD iter. 103/499: loss=9.074560584653934, w0=68.64851485148525, w1=23.632733215575744\n",
      "SubGD iter. 104/499: loss=9.010190533814978, w0=68.91188118811891, w1=23.723398926044382\n",
      "SubGD iter. 105/499: loss=8.965004590334704, w0=69.16831683168327, w1=23.81167869676385\n",
      "SubGD iter. 106/499: loss=8.923653436063262, w0=69.40396039603971, w1=23.89280064823579\n",
      "SubGD iter. 107/499: loss=8.894388113128446, w0=69.62574257425753, w1=23.969150720209385\n",
      "SubGD iter. 108/499: loss=8.878611182129005, w0=69.81980198019812, w1=24.03595703318628\n",
      "SubGD iter. 109/499: loss=8.870361577226147, w0=69.98613861386148, w1=24.093219587166473\n",
      "SubGD iter. 110/499: loss=8.866528707034991, w0=70.14554455445554, w1=24.148096201397493\n",
      "SubGD iter. 111/499: loss=8.865298278696658, w0=70.29801980198029, w1=24.20058687587934\n",
      "SubGD iter. 112/499: loss=8.864663219792593, w0=70.45049504950504, w1=24.253077550361184\n",
      "SubGD iter. 113/499: loss=8.864028160888529, w0=70.60297029702978, w1=24.30556822484303\n",
      "SubGD iter. 114/499: loss=8.86496930161827, w0=70.74851485148523, w1=24.3556729595757\n",
      "SubGD iter. 115/499: loss=8.867438924669898, w0=70.88712871287136, w1=24.403391754559195\n",
      "SubGD iter. 116/499: loss=8.872116368408808, w0=71.01881188118818, w1=24.448724609793516\n",
      "SubGD iter. 117/499: loss=8.878741698959796, w0=71.1366336633664, w1=24.489285585529487\n",
      "SubGD iter. 118/499: loss=8.886633190968684, w0=71.240594059406, w1=24.52507468176711\n",
      "SubGD iter. 119/499: loss=8.897182743796165, w0=71.3237623762377, w1=24.553705958757206\n",
      "SubGD iter. 120/499: loss=8.906942532428967, w0=71.40000000000008, w1=24.57995129599813\n",
      "SubGD iter. 121/499: loss=8.916184182421679, w0=71.47623762376246, w1=24.60619663323905\n",
      "SubGD iter. 122/499: loss=8.925425832414392, w0=71.55247524752484, w1=24.632441970479974\n",
      "SubGD iter. 123/499: loss=8.934667482407104, w0=71.62871287128722, w1=24.658687307720896\n",
      "SubGD iter. 124/499: loss=8.944079881190905, w0=71.69108910891099, w1=24.68016076546347\n",
      "SubGD iter. 125/499: loss=8.953059245669017, w0=71.74653465346545, w1=24.699248283456868\n",
      "SubGD iter. 126/499: loss=8.961794953661846, w0=71.7950495049506, w1=24.71594986170109\n",
      "SubGD iter. 127/499: loss=8.970191844715716, w0=71.83663366336644, w1=24.73026550019614\n",
      "SubGD iter. 128/499: loss=8.977556037231425, w0=71.87821782178229, w1=24.74458113869119\n",
      "SubGD iter. 129/499: loss=8.984920229747134, w0=71.91980198019813, w1=24.758896777186237\n",
      "SubGD iter. 130/499: loss=8.992284422262843, w0=71.96138613861397, w1=24.773212415681286\n",
      "SubGD iter. 131/499: loss=8.99964861477855, w0=72.00297029702982, w1=24.787528054176335\n",
      "SubGD iter. 132/499: loss=9.00731126988347, w0=72.03762376237636, w1=24.79945775292221\n",
      "SubGD iter. 133/499: loss=9.014004398855871, w0=72.05841584158428, w1=24.806615572169733\n",
      "SubGD iter. 134/499: loss=9.018396693003925, w0=72.0792079207922, w1=24.813773391417257\n",
      "SubGD iter. 135/499: loss=9.02278898715198, w0=72.10000000000012, w1=24.82093121066478\n",
      "SubGD iter. 136/499: loss=9.027181281300034, w0=72.12079207920804, w1=24.828089029912306\n",
      "SubGD iter. 137/499: loss=9.031573575448089, w0=72.14158415841597, w1=24.83524684915983\n",
      "SubGD iter. 138/499: loss=9.035965869596142, w0=72.16237623762389, w1=24.842404668407355\n",
      "SubGD iter. 139/499: loss=9.040358163744196, w0=72.18316831683181, w1=24.84956248765488\n",
      "SubGD iter. 140/499: loss=9.04475045789225, w0=72.20396039603973, w1=24.856720306902403\n",
      "SubGD iter. 141/499: loss=9.049142752040305, w0=72.22475247524766, w1=24.863878126149928\n",
      "SubGD iter. 142/499: loss=9.053691816505495, w0=72.23861386138627, w1=24.868650005648277\n",
      "SubGD iter. 143/499: loss=9.056731249390714, w0=72.25247524752488, w1=24.873421885146627\n",
      "SubGD iter. 144/499: loss=9.059770682275934, w0=72.2663366336635, w1=24.878193764644976\n",
      "SubGD iter. 145/499: loss=9.062810115161154, w0=72.28019801980211, w1=24.882965644143326\n",
      "SubGD iter. 146/499: loss=9.065849548046375, w0=72.29405940594073, w1=24.887737523641675\n",
      "SubGD iter. 147/499: loss=9.068888980931593, w0=72.30792079207934, w1=24.892509403140025\n",
      "SubGD iter. 148/499: loss=9.071928413816813, w0=72.32178217821796, w1=24.897281282638374\n",
      "SubGD iter. 149/499: loss=9.074967846702034, w0=72.33564356435657, w1=24.902053162136724\n",
      "SubGD iter. 150/499: loss=9.078007279587254, w0=72.34950495049519, w1=24.906825041635074\n",
      "SubGD iter. 151/499: loss=9.081046712472473, w0=72.3633663366338, w1=24.911596921133423\n",
      "SubGD iter. 152/499: loss=9.084086145357693, w0=72.37722772277242, w1=24.916368800631773\n",
      "SubGD iter. 153/499: loss=9.087125578242912, w0=72.39108910891103, w1=24.921140680130122\n",
      "SubGD iter. 154/499: loss=9.090165011128132, w0=72.40495049504965, w1=24.925912559628472\n",
      "SubGD iter. 155/499: loss=9.093204444013354, w0=72.41881188118826, w1=24.93068443912682\n",
      "SubGD iter. 156/499: loss=9.096243876898573, w0=72.43267326732688, w1=24.93545631862517\n",
      "SubGD iter. 157/499: loss=9.099283309783793, w0=72.44653465346549, w1=24.94022819812352\n",
      "SubGD iter. 158/499: loss=9.102322742669013, w0=72.4603960396041, w1=24.94500007762187\n",
      "SubGD iter. 159/499: loss=9.105362175554232, w0=72.47425742574272, w1=24.94977195712022\n",
      "SubGD iter. 160/499: loss=9.108489907005636, w0=72.48118811881203, w1=24.952157896869394\n",
      "SubGD iter. 161/499: loss=9.11006653099663, w0=72.48811881188134, w1=24.95454383661857\n",
      "SubGD iter. 162/499: loss=9.111643154987627, w0=72.49504950495064, w1=24.956929776367744\n",
      "SubGD iter. 163/499: loss=9.113219778978623, w0=72.50198019801995, w1=24.95931571611692\n",
      "SubGD iter. 164/499: loss=9.114796402969619, w0=72.50891089108926, w1=24.961701655866094\n",
      "SubGD iter. 165/499: loss=9.116373026960616, w0=72.51584158415857, w1=24.96408759561527\n",
      "SubGD iter. 166/499: loss=9.117949650951614, w0=72.52277227722787, w1=24.966473535364443\n",
      "SubGD iter. 167/499: loss=9.11952627494261, w0=72.52970297029718, w1=24.968859475113618\n",
      "SubGD iter. 168/499: loss=9.121102898933605, w0=72.53663366336649, w1=24.971245414862793\n",
      "SubGD iter. 169/499: loss=9.122679522924601, w0=72.5435643564358, w1=24.973631354611967\n",
      "SubGD iter. 170/499: loss=9.124256146915599, w0=72.5504950495051, w1=24.976017294361142\n",
      "SubGD iter. 171/499: loss=9.125832770906595, w0=72.55742574257441, w1=24.978403234110317\n",
      "SubGD iter. 172/499: loss=9.12740939489759, w0=72.56435643564372, w1=24.980789173859492\n",
      "SubGD iter. 173/499: loss=9.128986018888588, w0=72.57128712871302, w1=24.983175113608667\n",
      "SubGD iter. 174/499: loss=9.130562642879585, w0=72.57821782178233, w1=24.98556105335784\n",
      "SubGD iter. 175/499: loss=9.132139266870581, w0=72.58514851485164, w1=24.987946993107016\n",
      "SubGD iter. 176/499: loss=9.133715890861577, w0=72.59207920792095, w1=24.99033293285619\n",
      "SubGD iter. 177/499: loss=9.135292514852575, w0=72.59900990099025, w1=24.992718872605366\n",
      "SubGD iter. 178/499: loss=9.13686913884357, w0=72.60594059405956, w1=24.99510481235454\n",
      "SubGD iter. 179/499: loss=9.138445762834566, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 180/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 181/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 182/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 183/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 184/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 185/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 186/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 187/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 188/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 189/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 190/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 191/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 192/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 193/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 194/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 195/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 196/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 197/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 198/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 199/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 200/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 201/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 202/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 203/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 204/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 205/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 206/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 207/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 208/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 209/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 210/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 211/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 212/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 213/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 214/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 215/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 216/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 217/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 218/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 219/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 220/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 221/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 222/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 223/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 224/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 225/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 226/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 227/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 228/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 229/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 230/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 231/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 232/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 233/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 234/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 235/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 236/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 237/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 238/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 239/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 240/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 241/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 242/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 243/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 244/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 245/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 246/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 247/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 248/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 249/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 250/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 251/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 252/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 253/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 254/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 255/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 256/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 257/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 258/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 259/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 260/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 261/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 262/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 263/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 264/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 265/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 266/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 267/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 268/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 269/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 270/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 271/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 272/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 273/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 274/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 275/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 276/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 277/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 278/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 279/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 280/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 281/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 282/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 283/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 284/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 285/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 286/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 287/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 288/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 289/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 290/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 291/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 292/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 293/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 294/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 295/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 296/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 297/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 298/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 299/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 300/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 301/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 302/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 303/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 304/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 305/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 306/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 307/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 308/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 309/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 310/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 311/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 312/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 313/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 314/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 315/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 316/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 317/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 318/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 319/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 320/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 321/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 322/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 323/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 324/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 325/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 326/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 327/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 328/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 329/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 330/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 331/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 332/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 333/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 334/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 335/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 336/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 337/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 338/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 339/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 340/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 341/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 342/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 343/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 344/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 345/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 346/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 347/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 348/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 349/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 350/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 351/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 352/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 353/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 354/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 355/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 356/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 357/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 358/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 359/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 360/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 361/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 362/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 363/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 364/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 365/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 366/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 367/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 368/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 369/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 370/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 371/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 372/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 373/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 374/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 375/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 376/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 377/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 378/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 379/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 380/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 381/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 382/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 383/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 384/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 385/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 386/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 387/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 388/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 389/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 390/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 391/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 392/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 393/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 394/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 395/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 396/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 397/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 398/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 399/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 400/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 401/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 402/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 403/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 404/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 405/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 406/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 407/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 408/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 409/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 410/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 411/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 412/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 413/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 414/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 415/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 416/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 417/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 418/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 419/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 420/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 421/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 422/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 423/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 424/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 425/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 426/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 427/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 428/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 429/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 430/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 431/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 432/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 433/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 434/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 435/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 436/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 437/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 438/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 439/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 440/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 441/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 442/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 443/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 444/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 445/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 446/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 447/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 448/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 449/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 450/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 451/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 452/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 453/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 454/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 455/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 456/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 457/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 458/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 459/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 460/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 461/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 462/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 463/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 464/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 465/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 466/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 467/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 468/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 469/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 470/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 471/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 472/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 473/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 474/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 475/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 476/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 477/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 478/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 479/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 480/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 481/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 482/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 483/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 484/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 485/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 486/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 487/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 488/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 489/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 490/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 491/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 492/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 493/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 494/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 495/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 496/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 497/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 498/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD iter. 499/499: loss=9.14007549561306, w0=72.61287128712887, w1=24.997490752103715\n",
      "SubGD: execution time=0.029 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bcf6aee27349dfbc5f366b5e33697d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            batch_tx = minibatch_tx\n",
    "            batch_y = minibatch_y\n",
    "            \n",
    "        gradient = compute_subgradient_mae(batch_y, batch_tx, w)\n",
    "\n",
    "        w = w - gamma*gradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=0.24097991466664814\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=0.4819598293332963\n",
      "SubSGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=0.7229397439999444\n",
      "SubSGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=0.9639196586665926\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=1.2048995733332406\n",
      "SubSGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=1.4458794879998886\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=1.6868594026665367\n",
      "SubSGD iter. 7/499: loss=69.16780585492637, w0=5.6000000000000005, w1=1.9278393173331847\n",
      "SubSGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=2.1688192319998327\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=2.4097991466664808\n",
      "SubSGD iter. 10/499: loss=67.06780585492636, w0=7.700000000000001, w1=2.650779061333129\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=2.891758975999777\n",
      "SubSGD iter. 12/499: loss=65.66780585492637, w0=9.1, w1=3.132738890666425\n",
      "SubSGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=3.373718805333073\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=3.614698719999721\n",
      "SubSGD iter. 15/499: loss=63.56780585492638, w0=11.199999999999998, w1=3.855678634666369\n",
      "SubSGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=4.096658549333017\n",
      "SubSGD iter. 17/499: loss=62.16780585492638, w0=12.599999999999996, w1=4.3376384639996655\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=4.578618378666314\n",
      "SubSGD iter. 19/499: loss=60.76780585492637, w0=13.999999999999995, w1=4.819598293332962\n",
      "SubSGD iter. 20/499: loss=60.06780585492637, w0=14.699999999999994, w1=5.060578207999611\n",
      "SubSGD iter. 21/499: loss=59.367805854926374, w0=15.399999999999993, w1=5.301558122666259\n",
      "SubSGD iter. 22/499: loss=58.66780585492637, w0=16.099999999999994, w1=5.542538037332908\n",
      "SubSGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=5.783517951999556\n",
      "SubSGD iter. 24/499: loss=57.26780585492637, w0=17.499999999999993, w1=6.024497866666205\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=6.265477781332853\n",
      "SubSGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=6.506457695999502\n",
      "SubSGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=6.74743761066615\n",
      "SubSGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=6.988417525332799\n",
      "SubSGD iter. 29/499: loss=53.76780585492637, w0=20.99999999999999, w1=7.229397439999447\n",
      "SubSGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=7.470377354666096\n",
      "SubSGD iter. 31/499: loss=52.367805854926374, w0=22.399999999999988, w1=7.711357269332744\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=7.952337183999393\n",
      "SubSGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=8.193317098666041\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=8.434297013332689\n",
      "SubSGD iter. 35/499: loss=49.567805854926384, w0=25.199999999999985, w1=8.675276927999336\n",
      "SubSGD iter. 36/499: loss=48.867805854926374, w0=25.899999999999984, w1=8.916256842665984\n",
      "SubSGD iter. 37/499: loss=48.167805854926385, w0=26.599999999999984, w1=9.157236757332631\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=9.398216671999279\n",
      "SubSGD iter. 39/499: loss=46.76780585492638, w0=27.999999999999982, w1=9.639196586665927\n",
      "SubSGD iter. 40/499: loss=46.067805854926384, w0=28.69999999999998, w1=9.880176501332574\n",
      "SubSGD iter. 41/499: loss=45.36780585492639, w0=29.39999999999998, w1=10.121156415999222\n",
      "SubSGD iter. 42/499: loss=44.667805854926385, w0=30.09999999999998, w1=10.36213633066587\n",
      "SubSGD iter. 43/499: loss=43.96780585492639, w0=30.79999999999998, w1=10.603116245332517\n",
      "SubSGD iter. 44/499: loss=43.267805854926394, w0=31.49999999999998, w1=10.844096159999165\n",
      "SubSGD iter. 45/499: loss=42.567805854926384, w0=32.19999999999998, w1=11.085076074665812\n",
      "SubSGD iter. 46/499: loss=41.867805854926374, w0=32.899999999999984, w1=11.32605598933246\n",
      "SubSGD iter. 47/499: loss=41.16780585492638, w0=33.59999999999999, w1=11.567035903999107\n",
      "SubSGD iter. 48/499: loss=40.467805854926375, w0=34.29999999999999, w1=11.808015818665755\n",
      "SubSGD iter. 49/499: loss=39.76780585492637, w0=34.99999999999999, w1=12.048995733332402\n",
      "SubSGD iter. 50/499: loss=39.06780585492637, w0=35.699999999999996, w1=12.28997564799905\n",
      "SubSGD iter. 51/499: loss=38.36780585492637, w0=36.4, w1=12.530955562665698\n",
      "SubSGD iter. 52/499: loss=37.667805854926364, w0=37.1, w1=12.771935477332345\n",
      "SubSGD iter. 53/499: loss=36.96780585492636, w0=37.800000000000004, w1=13.012915391998993\n",
      "SubSGD iter. 54/499: loss=36.26780585492636, w0=38.50000000000001, w1=13.25389530666564\n",
      "SubSGD iter. 55/499: loss=35.567805854926355, w0=39.20000000000001, w1=13.494875221332288\n",
      "SubSGD iter. 56/499: loss=34.86780585492635, w0=39.90000000000001, w1=13.735855135998936\n",
      "SubSGD iter. 57/499: loss=34.16780585492635, w0=40.600000000000016, w1=13.976835050665583\n",
      "SubSGD iter. 58/499: loss=33.46780585492635, w0=41.30000000000002, w1=14.21781496533223\n",
      "SubSGD iter. 59/499: loss=32.767805854926344, w0=42.00000000000002, w1=14.458794879998878\n",
      "SubSGD iter. 60/499: loss=32.06780585492634, w0=42.700000000000024, w1=14.699774794665526\n",
      "SubSGD iter. 61/499: loss=31.367805854926335, w0=43.40000000000003, w1=14.940754709332174\n",
      "SubSGD iter. 62/499: loss=30.667805854926332, w0=44.10000000000003, w1=15.181734623998821\n",
      "SubSGD iter. 63/499: loss=29.967805854926333, w0=44.80000000000003, w1=15.422714538665469\n",
      "SubSGD iter. 64/499: loss=29.26780585492633, w0=45.500000000000036, w1=15.663694453332116\n",
      "SubSGD iter. 65/499: loss=28.567805854926327, w0=46.20000000000004, w1=15.904674367998764\n",
      "SubSGD iter. 66/499: loss=27.867805854926324, w0=46.90000000000004, w1=16.14565428266541\n",
      "SubSGD iter. 67/499: loss=27.16780585492632, w0=47.600000000000044, w1=16.38663419733206\n",
      "SubSGD iter. 68/499: loss=26.46780585492632, w0=48.30000000000005, w1=16.62761411199871\n",
      "SubSGD iter. 69/499: loss=25.767805854926316, w0=49.00000000000005, w1=16.86859402666536\n",
      "SubSGD iter. 70/499: loss=25.067805854926313, w0=49.70000000000005, w1=17.10957394133201\n",
      "SubSGD iter. 71/499: loss=24.36780585492631, w0=50.400000000000055, w1=17.35055385599866\n",
      "SubSGD iter. 72/499: loss=23.667805854926307, w0=51.10000000000006, w1=17.591533770665308\n",
      "SubSGD iter. 73/499: loss=22.9678058549263, w0=51.80000000000006, w1=17.832513685331957\n",
      "SubSGD iter. 74/499: loss=22.267805854926298, w0=52.500000000000064, w1=18.073493599998606\n",
      "SubSGD iter. 75/499: loss=21.567805854926295, w0=53.20000000000007, w1=18.314473514665256\n",
      "SubSGD iter. 76/499: loss=20.867805854926285, w0=53.90000000000007, w1=18.555453429331905\n",
      "SubSGD iter. 77/499: loss=20.16780585492629, w0=54.60000000000007, w1=18.796433343998554\n",
      "SubSGD iter. 78/499: loss=19.467805854926286, w0=55.300000000000075, w1=19.037413258665204\n",
      "SubSGD iter. 79/499: loss=18.767805854926284, w0=56.00000000000008, w1=19.278393173331853\n",
      "SubSGD iter. 80/499: loss=18.067805854926277, w0=56.70000000000008, w1=19.519373087998503\n",
      "SubSGD iter. 81/499: loss=17.367805854926274, w0=57.400000000000084, w1=19.760353002665152\n",
      "SubSGD iter. 82/499: loss=16.66780585492627, w0=58.10000000000009, w1=20.0013329173318\n",
      "SubSGD iter. 83/499: loss=15.972537657946242, w0=58.80000000000009, w1=20.24231283199845\n",
      "SubSGD iter. 84/499: loss=15.291581197963648, w0=59.50000000000009, w1=20.4832927466651\n",
      "SubSGD iter. 85/499: loss=14.629889024920281, w0=60.200000000000095, w1=20.72427266133175\n",
      "SubSGD iter. 86/499: loss=13.982888333697517, w0=60.9000000000001, w1=20.9652525759984\n",
      "SubSGD iter. 87/499: loss=13.3651404036408, w0=61.6000000000001, w1=21.206232490665048\n",
      "SubSGD iter. 88/499: loss=12.782471037033927, w0=62.300000000000104, w1=21.447212405331697\n",
      "SubSGD iter. 89/499: loss=12.222186564247687, w0=63.00000000000011, w1=21.688192319998347\n",
      "SubSGD iter. 90/499: loss=11.707531262518355, w0=63.70000000000011, w1=21.929172234664996\n",
      "SubSGD iter. 91/499: loss=11.230760085492115, w0=64.4000000000001, w1=22.170152149331646\n",
      "SubSGD iter. 92/499: loss=10.763656825438861, w0=63.7000000000001, w1=21.929172234664996\n",
      "SubSGD iter. 93/499: loss=11.230760085492122, w0=64.4000000000001, w1=22.170152149331646\n",
      "SubSGD iter. 94/499: loss=10.763656825438861, w0=65.10000000000011, w1=22.411132063998295\n",
      "SubSGD iter. 95/499: loss=10.358644885416828, w0=64.4000000000001, w1=22.170152149331646\n",
      "SubSGD iter. 96/499: loss=10.763656825438861, w0=65.10000000000011, w1=22.411132063998295\n",
      "SubSGD iter. 97/499: loss=10.358644885416828, w0=65.80000000000011, w1=22.652111978664944\n",
      "SubSGD iter. 98/499: loss=10.006250610294305, w0=66.50000000000011, w1=22.893091893331594\n",
      "SubSGD iter. 99/499: loss=9.700498403213363, w0=67.20000000000012, w1=23.134071807998243\n",
      "SubSGD iter. 100/499: loss=9.430004727504048, w0=66.50000000000011, w1=22.893091893331594\n",
      "SubSGD iter. 101/499: loss=9.700498403213363, w0=65.80000000000011, w1=22.652111978664944\n",
      "SubSGD iter. 102/499: loss=10.006250610294305, w0=66.50000000000011, w1=22.893091893331594\n",
      "SubSGD iter. 103/499: loss=9.700498403213363, w0=65.80000000000011, w1=22.652111978664944\n",
      "SubSGD iter. 104/499: loss=10.006250610294305, w0=65.10000000000011, w1=22.411132063998295\n",
      "SubSGD iter. 105/499: loss=10.358644885416828, w0=65.80000000000011, w1=22.652111978664944\n",
      "SubSGD iter. 106/499: loss=10.006250610294305, w0=66.50000000000011, w1=22.893091893331594\n",
      "SubSGD iter. 107/499: loss=9.700498403213363, w0=67.20000000000012, w1=23.134071807998243\n",
      "SubSGD iter. 108/499: loss=9.430004727504048, w0=67.90000000000012, w1=23.375051722664892\n",
      "SubSGD iter. 109/499: loss=9.19906091987911, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 110/499: loss=9.019347980627948, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 111/499: loss=8.906419763713156, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 112/499: loss=9.019347980627948, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 113/499: loss=8.906419763713156, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 114/499: loss=9.019347980627948, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 115/499: loss=8.906419763713156, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 116/499: loss=9.019347980627948, w0=67.90000000000012, w1=23.375051722664892\n",
      "SubSGD iter. 117/499: loss=9.19906091987911, w0=67.20000000000012, w1=23.134071807998243\n",
      "SubSGD iter. 118/499: loss=9.430004727504048, w0=67.90000000000012, w1=23.375051722664892\n",
      "SubSGD iter. 119/499: loss=9.19906091987911, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 120/499: loss=9.019347980627948, w0=67.90000000000012, w1=23.375051722664892\n",
      "SubSGD iter. 121/499: loss=9.19906091987911, w0=67.20000000000012, w1=23.134071807998243\n",
      "SubSGD iter. 122/499: loss=9.430004727504048, w0=66.50000000000011, w1=22.893091893331594\n",
      "SubSGD iter. 123/499: loss=9.700498403213363, w0=67.20000000000012, w1=23.134071807998243\n",
      "SubSGD iter. 124/499: loss=9.430004727504048, w0=67.90000000000012, w1=23.375051722664892\n",
      "SubSGD iter. 125/499: loss=9.19906091987911, w0=67.20000000000012, w1=23.134071807998243\n",
      "SubSGD iter. 126/499: loss=9.430004727504048, w0=67.90000000000012, w1=23.375051722664892\n",
      "SubSGD iter. 127/499: loss=9.19906091987911, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 128/499: loss=9.019347980627948, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 129/499: loss=8.906419763713156, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 130/499: loss=9.019347980627948, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 131/499: loss=8.906419763713156, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 132/499: loss=9.019347980627948, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 133/499: loss=8.906419763713156, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 134/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 135/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 136/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 137/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 138/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 139/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 140/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 141/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 142/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 143/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 144/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 145/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 146/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 147/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 148/499: loss=10.11102417485309, w0=76.30000000000015, w1=26.266810698664685\n",
      "SubSGD iter. 149/499: loss=10.42919218725816, w0=77.00000000000016, w1=26.507790613331334\n",
      "SubSGD iter. 150/499: loss=10.791441874269449, w0=76.30000000000015, w1=26.266810698664685\n",
      "SubSGD iter. 151/499: loss=10.42919218725816, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 152/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 153/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 154/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 155/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 156/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 157/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 158/499: loss=10.11102417485309, w0=76.30000000000015, w1=26.266810698664685\n",
      "SubSGD iter. 159/499: loss=10.42919218725816, w0=77.00000000000016, w1=26.507790613331334\n",
      "SubSGD iter. 160/499: loss=10.791441874269449, w0=77.70000000000016, w1=26.748770527997983\n",
      "SubSGD iter. 161/499: loss=11.177324034215548, w0=78.40000000000016, w1=26.989750442664633\n",
      "SubSGD iter. 162/499: loss=11.603521502422657, w0=79.10000000000016, w1=27.230730357331282\n",
      "SubSGD iter. 163/499: loss=12.061769486422532, w0=78.40000000000016, w1=26.989750442664633\n",
      "SubSGD iter. 164/499: loss=11.603521502422657, w0=79.10000000000016, w1=27.230730357331282\n",
      "SubSGD iter. 165/499: loss=12.061769486422532, w0=78.40000000000016, w1=26.989750442664633\n",
      "SubSGD iter. 166/499: loss=11.603521502422657, w0=79.10000000000016, w1=27.230730357331282\n",
      "SubSGD iter. 167/499: loss=12.061769486422532, w0=78.40000000000016, w1=26.989750442664633\n",
      "SubSGD iter. 168/499: loss=11.603521502422657, w0=77.70000000000016, w1=26.748770527997983\n",
      "SubSGD iter. 169/499: loss=11.177324034215548, w0=77.00000000000016, w1=26.507790613331334\n",
      "SubSGD iter. 170/499: loss=10.791441874269449, w0=77.70000000000016, w1=26.748770527997983\n",
      "SubSGD iter. 171/499: loss=11.177324034215548, w0=78.40000000000016, w1=26.989750442664633\n",
      "SubSGD iter. 172/499: loss=11.603521502422657, w0=77.70000000000016, w1=26.748770527997983\n",
      "SubSGD iter. 173/499: loss=11.177324034215548, w0=77.00000000000016, w1=26.507790613331334\n",
      "SubSGD iter. 174/499: loss=10.791441874269449, w0=76.30000000000015, w1=26.266810698664685\n",
      "SubSGD iter. 175/499: loss=10.42919218725816, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 176/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 177/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 178/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 179/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 180/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 181/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 182/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 183/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 184/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 185/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 186/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 187/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 188/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 189/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 190/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 191/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 192/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 193/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 194/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 195/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 196/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 197/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 198/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 199/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 200/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 201/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 202/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 203/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 204/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 205/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 206/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 207/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 208/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 209/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 210/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 211/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 212/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 213/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 214/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 215/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 216/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 217/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 218/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 219/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 220/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 221/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 222/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 223/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 224/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 225/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 226/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 227/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 228/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 229/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 230/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 231/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 232/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 233/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 234/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 235/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 236/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 237/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 238/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 239/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 240/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 241/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 242/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 243/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 244/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 245/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 246/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 247/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 248/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 249/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 250/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 251/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 252/499: loss=8.866350203492287, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 253/499: loss=8.906419763713156, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 254/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 255/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 256/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 257/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 258/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 259/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 260/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 261/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 262/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 263/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 264/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 265/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 266/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 267/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 268/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 269/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 270/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 271/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 272/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 273/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 274/499: loss=10.11102417485309, w0=76.30000000000015, w1=26.266810698664685\n",
      "SubSGD iter. 275/499: loss=10.42919218725816, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 276/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 277/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 278/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 279/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 280/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 281/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 282/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 283/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 284/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 285/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 286/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 287/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 288/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 289/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 290/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 291/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 292/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 293/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 294/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 295/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 296/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 297/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 298/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 299/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 300/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 301/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 302/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 303/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 304/499: loss=8.866350203492287, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 305/499: loss=8.906419763713156, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 306/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 307/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 308/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 309/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 310/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 311/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 312/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 313/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 314/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 315/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 316/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 317/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 318/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 319/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 320/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 321/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 322/499: loss=8.866350203492287, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 323/499: loss=8.906419763713156, w0=68.60000000000012, w1=23.61603163733154\n",
      "SubSGD iter. 324/499: loss=9.019347980627948, w0=69.30000000000013, w1=23.85701155199819\n",
      "SubSGD iter. 325/499: loss=8.906419763713156, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 326/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 327/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 328/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 329/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 330/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 331/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 332/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 333/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 334/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 335/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 336/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 337/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 338/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 339/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 340/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 341/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 342/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 343/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 344/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 345/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 346/499: loss=10.11102417485309, w0=76.30000000000015, w1=26.266810698664685\n",
      "SubSGD iter. 347/499: loss=10.42919218725816, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 348/499: loss=10.11102417485309, w0=76.30000000000015, w1=26.266810698664685\n",
      "SubSGD iter. 349/499: loss=10.42919218725816, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 350/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 351/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 352/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 353/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 354/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 355/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 356/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 357/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 358/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 359/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 360/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 361/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 362/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 363/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 364/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 365/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 366/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 367/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 368/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 369/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 370/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 371/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 372/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 373/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 374/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 375/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 376/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 377/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 378/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 379/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 380/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 381/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 382/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 383/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 384/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 385/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 386/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 387/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 388/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 389/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 390/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 391/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 392/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 393/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 394/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 395/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 396/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 397/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 398/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 399/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 400/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 401/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 402/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 403/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 404/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 405/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 406/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 407/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 408/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 409/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 410/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 411/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 412/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 413/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 414/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 415/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 416/499: loss=10.11102417485309, w0=76.30000000000015, w1=26.266810698664685\n",
      "SubSGD iter. 417/499: loss=10.42919218725816, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 418/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 419/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 420/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 421/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 422/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 423/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 424/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 425/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 426/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 427/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 428/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 429/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 430/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 431/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 432/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 433/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 434/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 435/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 436/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 437/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 438/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 439/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 440/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 441/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 442/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 443/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 444/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 445/499: loss=9.374515400251203, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 446/499: loss=9.59501226893037, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 447/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 448/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 449/499: loss=9.839014051663996, w0=75.60000000000015, w1=26.025830783998035\n",
      "SubSGD iter. 450/499: loss=10.11102417485309, w0=74.90000000000015, w1=25.784850869331386\n",
      "SubSGD iter. 451/499: loss=9.839014051663996, w0=74.20000000000014, w1=25.543870954664737\n",
      "SubSGD iter. 452/499: loss=9.59501226893037, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 453/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 454/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 455/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 456/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 457/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 458/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 459/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 460/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 461/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 462/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 463/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 464/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 465/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 466/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 467/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 468/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 469/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 470/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 471/499: loss=8.866121741676922, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 472/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 473/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 474/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 475/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 476/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 477/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 478/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 479/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 480/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 481/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 482/499: loss=9.185419322546766, w0=73.50000000000014, w1=25.302891039998087\n",
      "SubSGD iter. 483/499: loss=9.374515400251203, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 484/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 485/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 486/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 487/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 488/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 489/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 490/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 491/499: loss=9.027181281300038, w0=72.80000000000014, w1=25.061911125331438\n",
      "SubSGD iter. 492/499: loss=9.185419322546766, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 493/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 494/499: loss=8.91618418242168, w0=72.10000000000014, w1=24.82093121066479\n",
      "SubSGD iter. 495/499: loss=9.027181281300038, w0=71.40000000000013, w1=24.57995129599814\n",
      "SubSGD iter. 496/499: loss=8.91618418242168, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 497/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD iter. 498/499: loss=8.866350203492287, w0=70.70000000000013, w1=24.33897138133149\n",
      "SubSGD iter. 499/499: loss=8.866121741676922, w0=70.00000000000013, w1=24.09799146666484\n",
      "SubSGD: execution time=0.042 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36468293cfad48cd8bf2c540e942ef0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
